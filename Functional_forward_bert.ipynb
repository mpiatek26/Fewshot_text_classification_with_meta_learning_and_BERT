{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkRTHOAPDNjDHg3ECwfaR3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7efdc2749be641b683321fb25d39134c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9b2413ff31744eb81593ec8a70a33ef",
              "IPY_MODEL_5b330bf1d79c435d8f82030435ea0cc7",
              "IPY_MODEL_e9a18705e2f349dd9d1542b39333d2d6"
            ],
            "layout": "IPY_MODEL_17f09876c1644c28b2c6a2998be57893"
          }
        },
        "b9b2413ff31744eb81593ec8a70a33ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fdd330a5a6845dd81021d62f88ac3dd",
            "placeholder": "​",
            "style": "IPY_MODEL_8172a045de2a4c7284fa3947f0ad5289",
            "value": "config.json: 100%"
          }
        },
        "5b330bf1d79c435d8f82030435ea0cc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20e85a44e5d14259b91c3d35505d5e3a",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7e901d6864e4ae88bb105bbd67456f3",
            "value": 570
          }
        },
        "e9a18705e2f349dd9d1542b39333d2d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10bae7e69a0544d5a64eafb5050fa6d6",
            "placeholder": "​",
            "style": "IPY_MODEL_3b81dc6129b84c20858d9b3ede0bd061",
            "value": " 570/570 [00:00&lt;00:00, 7.59kB/s]"
          }
        },
        "17f09876c1644c28b2c6a2998be57893": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fdd330a5a6845dd81021d62f88ac3dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8172a045de2a4c7284fa3947f0ad5289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20e85a44e5d14259b91c3d35505d5e3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7e901d6864e4ae88bb105bbd67456f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10bae7e69a0544d5a64eafb5050fa6d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b81dc6129b84c20858d9b3ede0bd061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87cc6bcc155b4ed6808356f82c87eba2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac0b4f1e43fd4e35be702207b63b989a",
              "IPY_MODEL_f417932d89384481b31ab9112dcab78e",
              "IPY_MODEL_a90a1e8e95084ae08d4f3053b4cc274b"
            ],
            "layout": "IPY_MODEL_b43d321395db4533857020b85af0e97a"
          }
        },
        "ac0b4f1e43fd4e35be702207b63b989a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d4795fcdbff4d4aabf435f49fb922a3",
            "placeholder": "​",
            "style": "IPY_MODEL_a61dd939abf04fe5a33125ba1afa9788",
            "value": "model.safetensors: 100%"
          }
        },
        "f417932d89384481b31ab9112dcab78e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_070bed435b3a4058b23eb2f05f5b335f",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6e1dc826c60416bb7b6c4f6121974da",
            "value": 440449768
          }
        },
        "a90a1e8e95084ae08d4f3053b4cc274b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0508d7b243d041ca96547a243bd76339",
            "placeholder": "​",
            "style": "IPY_MODEL_fca6279fdeff4741a28d8e8e13d5d5a8",
            "value": " 440M/440M [00:05&lt;00:00, 120MB/s]"
          }
        },
        "b43d321395db4533857020b85af0e97a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d4795fcdbff4d4aabf435f49fb922a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a61dd939abf04fe5a33125ba1afa9788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "070bed435b3a4058b23eb2f05f5b335f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6e1dc826c60416bb7b6c4f6121974da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0508d7b243d041ca96547a243bd76339": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fca6279fdeff4741a28d8e8e13d5d5a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mpiatek26/Fewshot_text_classification_with_meta_learning_and_BERT/blob/main/Functional_forward_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422,
          "referenced_widgets": [
            "7efdc2749be641b683321fb25d39134c",
            "b9b2413ff31744eb81593ec8a70a33ef",
            "5b330bf1d79c435d8f82030435ea0cc7",
            "e9a18705e2f349dd9d1542b39333d2d6",
            "17f09876c1644c28b2c6a2998be57893",
            "5fdd330a5a6845dd81021d62f88ac3dd",
            "8172a045de2a4c7284fa3947f0ad5289",
            "20e85a44e5d14259b91c3d35505d5e3a",
            "a7e901d6864e4ae88bb105bbd67456f3",
            "10bae7e69a0544d5a64eafb5050fa6d6",
            "3b81dc6129b84c20858d9b3ede0bd061",
            "87cc6bcc155b4ed6808356f82c87eba2",
            "ac0b4f1e43fd4e35be702207b63b989a",
            "f417932d89384481b31ab9112dcab78e",
            "a90a1e8e95084ae08d4f3053b4cc274b",
            "b43d321395db4533857020b85af0e97a",
            "0d4795fcdbff4d4aabf435f49fb922a3",
            "a61dd939abf04fe5a33125ba1afa9788",
            "070bed435b3a4058b23eb2f05f5b335f",
            "f6e1dc826c60416bb7b6c4f6121974da",
            "0508d7b243d041ca96547a243bd76339",
            "fca6279fdeff4741a28d8e8e13d5d5a8"
          ]
        },
        "id": "YP6cClneMVZA",
        "outputId": "186831f0-7b8b-452b-e318-d9d72dc5df40"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7efdc2749be641b683321fb25d39134c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87cc6bcc155b4ed6808356f82c87eba2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[[-0.6503,  0.4436,  0.0087,  ..., -0.5760,  0.7778,  0.7293],\n",
            "         [-0.1977,  0.3785,  0.4189,  ..., -0.1297,  0.1625,  0.5795],\n",
            "         [-0.0350,  0.6271, -0.0882,  ..., -0.3974,  0.0212,  0.5252],\n",
            "         ...,\n",
            "         [-0.0451,  0.0552,  0.2546,  ..., -0.0808,  0.2008,  0.1066],\n",
            "         [ 0.0305,  0.4843,  0.0541,  ...,  0.2033,  0.2690, -0.3234],\n",
            "         [ 0.3356,  0.0944, -0.7279,  ...,  0.2293, -0.7348, -0.1534]],\n",
            "\n",
            "        [[-0.1876, -0.0426,  0.2838,  ..., -0.2519,  0.5150,  0.2350],\n",
            "         [ 0.5045, -0.1870,  0.2053,  ..., -0.0788,  0.6224,  0.0272],\n",
            "         [ 1.0817,  0.1308,  0.0738,  ..., -0.4028,  0.7071,  0.1980],\n",
            "         ...,\n",
            "         [ 0.4304, -0.3091,  0.0377,  ..., -0.4928,  0.4463,  0.3686],\n",
            "         [-0.1695, -0.1630, -0.0579,  ...,  0.1627,  0.1821, -0.1245],\n",
            "         [ 0.3846,  0.2639, -0.5205,  ...,  0.0394, -0.7808, -0.2126]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>),)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import OrderedDict\n",
        "from torch.nn.functional import gelu, elu\n",
        "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "def functional_bert(\n",
        "    fast_weights, config, input_ids=None,\n",
        "    attention_mask=None, token_type_ids=None,\n",
        "    position_ids=None, head_mask=None,\n",
        "    inputs_embeds=None, encoder_hidden_states=None,\n",
        "    encoder_attention_mask=None, is_train = True\n",
        "):\n",
        "\n",
        "    if input_ids is not None and inputs_embeds is not None:\n",
        "        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "    elif input_ids is not None:\n",
        "        input_shape = input_ids.size()\n",
        "    elif inputs_embeds is not None:\n",
        "        input_shape = inputs_embeds.size()[:-1]\n",
        "    else:\n",
        "        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "    device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "    if attention_mask is None:\n",
        "        attention_mask = torch.ones(input_shape, device=device)\n",
        "    if token_type_ids is None:\n",
        "        token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "    if attention_mask.dim() == 3:\n",
        "        extended_attention_mask = attention_mask[:, None, :, :]\n",
        "    elif attention_mask.dim() == 2:\n",
        "        if config.is_decoder:\n",
        "            batch_size, seq_length = input_shape\n",
        "            seq_ids = torch.arange(seq_length, device=device)\n",
        "            causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n",
        "            causal_mask = causal_mask.to(torch.long)\n",
        "            extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
        "        else:\n",
        "            extended_attention_mask = attention_mask[:, None, None, :]\n",
        "    else:\n",
        "        raise ValueError(\"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(input_shape, attention_mask.shape))\n",
        "\n",
        "    extended_attention_mask = extended_attention_mask.to(dtype=next((p for p in fast_weights.values())).dtype)  # fp16 compatibility\n",
        "    extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "    if config.is_decoder and encoder_hidden_states is not None:\n",
        "        encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "        if encoder_attention_mask is None:\n",
        "            encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "\n",
        "        if encoder_attention_mask.dim() == 3:\n",
        "            encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n",
        "        elif encoder_attention_mask.dim() == 2:\n",
        "            encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n",
        "        else:\n",
        "            raise ValueError(\"Wrong shape for encoder_hidden_shape (shape {}) or encoder_attention_mask (shape {})\".format(encoder_hidden_shape,\n",
        "                                                                                                                           encoder_attention_mask.shape))\n",
        "        encoder_extended_attention_mask = encoder_extended_attention_mask.to(dtype=next((p for p in fast_weights.values())).dtype)\n",
        "        encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -10000.0\n",
        "    else:\n",
        "        encoder_extended_attention_mask = None\n",
        "\n",
        "    if head_mask is not None:\n",
        "        if head_mask.dim() == 1:\n",
        "            head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
        "            head_mask = head_mask.expand(config.num_hidden_layers, -1, -1, -1, -1)\n",
        "        elif head_mask.dim() == 2:\n",
        "            head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n",
        "        head_mask = head_mask.to(dtype=next((p for p in fast_weights.values())).dtype)\n",
        "    else:\n",
        "        head_mask = [None] * config.num_hidden_layers\n",
        "\n",
        "    embedding_output = functional_embeeding(fast_weights, config, input_ids, position_ids,\n",
        "                                            token_type_ids, inputs_embeds, is_train = is_train)\n",
        "\n",
        "    encoder_outputs = functional_encoder(fast_weights, config, embedding_output,\n",
        "                                   attention_mask=extended_attention_mask,\n",
        "                                   head_mask=head_mask, encoder_hidden_states=encoder_hidden_states,\n",
        "                                   encoder_attention_mask=encoder_extended_attention_mask, is_train = is_train)\n",
        "\n",
        "    sequence_output = encoder_outputs\n",
        "    outputs = (sequence_output,)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def functional_embeeding(\n",
        "    fast_weights, config, input_ids, position_ids,\n",
        "    token_type_ids, inputs_embeds = None, is_train = True\n",
        "):\n",
        "\n",
        "    if input_ids is not None:\n",
        "        input_shape = input_ids.size()\n",
        "    else:\n",
        "        input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "    seq_length = input_shape[1]\n",
        "    device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "    if position_ids is None:\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand(input_shape)\n",
        "    if token_type_ids is None:\n",
        "        token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "    if inputs_embeds is None:\n",
        "        inputs_embeds = F.embedding(input_ids, fast_weights['bert.embeddings.word_embeddings.weight'], padding_idx = 0)\n",
        "\n",
        "    position_embeddings = F.embedding(position_ids, fast_weights['bert.embeddings.position_embeddings.weight'])\n",
        "    token_type_embeddings = F.embedding(token_type_ids, fast_weights['bert.embeddings.token_type_embeddings.weight'])\n",
        "\n",
        "    embeddings = inputs_embeds + position_embeddings + token_type_embeddings\n",
        "\n",
        "    embeddings = F.layer_norm(embeddings, [config.hidden_size],\n",
        "                              weight=fast_weights['bert.embeddings.LayerNorm.weight'],\n",
        "                              bias=fast_weights['bert.embeddings.LayerNorm.bias'],\n",
        "                              eps=config.layer_norm_eps)\n",
        "\n",
        "    embeddings = F.dropout(embeddings, p=config.hidden_dropout_prob, training = is_train)\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def transpose_for_scores(config, x):\n",
        "    new_x_shape = x.size()[:-1] + (config.num_attention_heads, int(config.hidden_size / config.num_attention_heads))\n",
        "    x = x.view(*new_x_shape)\n",
        "    return x.permute(0, 2, 1, 3)\n",
        "\n",
        "def functional_self_attention(\n",
        "    fast_weights, config, layer_idx, hidden_states,\n",
        "    attention_mask, head_mask, encoder_hidden_states,\n",
        "    encoder_attention_mask, is_train = True\n",
        "):\n",
        "\n",
        "    attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "    all_head_size = config.num_attention_heads * attention_head_size\n",
        "\n",
        "    mixed_query_layer = F.linear(hidden_states,\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.query.weight'],\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.query.bias'])\n",
        "\n",
        "    if encoder_hidden_states is not None:\n",
        "        mixed_key_layer = F.linear(encoder_hidden_states,\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.key.weight'],\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.key.bias'])\n",
        "        mixed_value_layer = F.linear(encoder_hidden_states,\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.value.weight'],\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.value.bias'])\n",
        "        attention_mask = encoder_attention_mask\n",
        "    else:\n",
        "        mixed_key_layer   = F.linear(hidden_states,\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.key.weight'],\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.key.bias'])\n",
        "        mixed_value_layer = F.linear(hidden_states,\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.value.weight'],\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.value.bias'])\n",
        "\n",
        "    query_layer = transpose_for_scores(config, mixed_query_layer)\n",
        "    key_layer   = transpose_for_scores(config, mixed_key_layer)\n",
        "    value_layer = transpose_for_scores(config, mixed_value_layer)\n",
        "\n",
        "    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "    attention_scores = attention_scores / math.sqrt(attention_head_size)\n",
        "    if attention_mask is not None:\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "    attention_probs = torch.nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "    if is_train:\n",
        "        attention_probs = F.dropout(attention_probs, p= config.attention_probs_dropout_prob)\n",
        "\n",
        "    # Mask heads if we want to\n",
        "    if head_mask is not None:\n",
        "        attention_probs = attention_probs * head_mask\n",
        "\n",
        "    context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "    context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "    new_context_layer_shape = context_layer.size()[:-2] + (all_head_size,)\n",
        "    context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "    outputs = context_layer\n",
        "    return outputs\n",
        "\n",
        "def functional_out_attention(\n",
        "    fast_weights, config,\n",
        "    layer_idx, hidden_states,\n",
        "    input_tensor, is_train = True\n",
        "):\n",
        "\n",
        "    hidden_states = F.linear(hidden_states,\n",
        "                            fast_weights['bert.encoder.layer.'+layer_idx+'.attention.output.dense.weight'],\n",
        "                            fast_weights['bert.encoder.layer.'+layer_idx+'.attention.output.dense.bias'])\n",
        "\n",
        "    hidden_states = F.dropout(hidden_states, p=config.hidden_dropout_prob, training = is_train)\n",
        "    hidden_states = F.layer_norm(hidden_states + input_tensor, [config.hidden_size],\n",
        "                              weight=fast_weights['bert.encoder.layer.'+layer_idx+'.attention.output.LayerNorm.weight'],\n",
        "                              bias=fast_weights['bert.encoder.layer.'+layer_idx+'.attention.output.LayerNorm.bias'],\n",
        "                              eps=config.layer_norm_eps)\n",
        "\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "def functional_attention(\n",
        "    fast_weights, config, layer_idx, hidden_states,\n",
        "    attention_mask=None, head_mask=None, encoder_hidden_states=None,\n",
        "    encoder_attention_mask=None, is_train = True\n",
        "):\n",
        "\n",
        "    self_outputs = functional_self_attention(fast_weights, config, layer_idx,\n",
        "                                             hidden_states, attention_mask, head_mask,\n",
        "                                             encoder_hidden_states, encoder_attention_mask, is_train)\n",
        "\n",
        "    attention_output = functional_out_attention(fast_weights, config, layer_idx,\n",
        "                                                self_outputs, hidden_states, is_train)\n",
        "    return attention_output\n",
        "\n",
        "def functional_intermediate(fast_weights, config, layer_idx, hidden_states, is_train = True):\n",
        "    weight_name = 'bert.encoder.layer.' + layer_idx + '.intermediate.dense.weight'\n",
        "    bias_name   = 'bert.encoder.layer.' + layer_idx + '.intermediate.dense.bias'\n",
        "    hidden_states = F.linear(hidden_states, fast_weights[weight_name], fast_weights[bias_name])\n",
        "    hidden_states = gelu(hidden_states)\n",
        "\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "def functional_output(fast_weights, config, layer_idx, hidden_states, input_tensor, is_train = True):\n",
        "\n",
        "    hidden_states = F.linear(hidden_states,\n",
        "                             fast_weights['bert.encoder.layer.'+layer_idx+'.output.dense.weight'],\n",
        "                             fast_weights['bert.encoder.layer.'+layer_idx+'.output.dense.bias'])\n",
        "\n",
        "    hidden_states = F.dropout(hidden_states, p=config.hidden_dropout_prob, training = is_train)\n",
        "    hidden_states = F.layer_norm(hidden_states + input_tensor, [config.hidden_size],\n",
        "                              weight=fast_weights['bert.encoder.layer.'+layer_idx+'.output.LayerNorm.weight'],\n",
        "                              bias=fast_weights['bert.encoder.layer.'+layer_idx+'.output.LayerNorm.bias'],\n",
        "                              eps=config.layer_norm_eps)\n",
        "    return hidden_states\n",
        "\n",
        "def functional_layer(\n",
        "    fast_weights, config, layer_idx, hidden_states, attention_mask,\n",
        "    head_mask, encoder_hidden_states, encoder_attention_mask, is_train = True\n",
        "):\n",
        "\n",
        "    self_attention_outputs = functional_attention(fast_weights, config, layer_idx,\n",
        "                                                  hidden_states, attention_mask, head_mask,\n",
        "                                                  encoder_hidden_states, encoder_attention_mask,is_train)\n",
        "\n",
        "    attention_output = self_attention_outputs\n",
        "    intermediate_output = functional_intermediate(fast_weights, config, layer_idx, attention_output, is_train)\n",
        "    layer_output = functional_output(fast_weights, config, layer_idx,\n",
        "                                     intermediate_output, attention_output, is_train)\n",
        "\n",
        "    return layer_output\n",
        "\n",
        "\n",
        "def functional_encoder(\n",
        "    fast_weights, config , hidden_states, attention_mask,\n",
        "    head_mask, encoder_hidden_states, encoder_attention_mask, is_train = True\n",
        "):\n",
        "\n",
        "    for i in range(0,config.num_hidden_layers):\n",
        "        layer_outputs = functional_layer(fast_weights, config, str(i),\n",
        "                                         hidden_states, attention_mask, head_mask[i],\n",
        "                                         encoder_hidden_states, encoder_attention_mask, is_train)\n",
        "        hidden_states = layer_outputs\n",
        "\n",
        "    outputs = hidden_states\n",
        "    return outputs\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "    fast_weights = OrderedDict(model.named_parameters())\n",
        "\n",
        "    input_ids = torch.Tensor([[  101,  1303,  1110,  1199,  3087,  1106,  4035, 13775,   102],\n",
        "                              [  101,   178,  1274,  1204,  1176,  1115,  4170,   182,   102]]).to(torch.long)\n",
        "    token_type_ids = torch.Tensor([[0,  0,  0,  0,  0,  1,  1, 1, 1],\n",
        "                                   [0,  0,  0,  0,  0,  1,  1, 1, 1]]).to(torch.long)\n",
        "    attention_mask = torch.Tensor([[1,  1,  1,  1,  1,  1,  1, 1, 1],\n",
        "                                   [1,  1,  1,  1,  1,  1,  1, 1, 1]]).to(torch.long)\n",
        "\n",
        "    print(functional_bert(fast_weights, model.config, input_ids=input_ids, attention_mask=attention_mask,\n",
        "                    token_type_ids=token_type_ids,is_train = True))\n",
        ""
      ]
    }
  ]
}