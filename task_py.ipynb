{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFTTeGerCesjJklXTCKc7o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mpiatek26/Fewshot_text_classification_with_meta_learning_and_BERT/blob/main/task_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lI8SqVFPiD_e"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "\n",
        "LABEL_MAP = {'positive': 0, 'negative': 1, 0: 'positive', 1: 'negative'}\n",
        "\n",
        "class MetaTask(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for creating meta-learning tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, examples, num_task, k_support, k_query, tokenizer, max_seq_length=256):\n",
        "        \"\"\"\n",
        "        Initializes the MetaTask dataset.\n",
        "\n",
        "        :param examples: List of examples, each example is a dict with keys 'domain', 'text', 'label'.\n",
        "        :param num_task: Number of training tasks.\n",
        "        :param k_support: Number of support samples per task.\n",
        "        :param k_query: Number of query samples per task.\n",
        "        :param tokenizer: Tokenizer for encoding the text.\n",
        "        :param max_seq_length: Maximum sequence length for tokenization.\n",
        "        \"\"\"\n",
        "        self.examples = examples\n",
        "        random.shuffle(self.examples)\n",
        "\n",
        "        self.num_task = num_task\n",
        "        self.k_support = k_support\n",
        "        self.k_query = k_query\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        self.supports, self.queries = self.create_batch(self.num_task)\n",
        "\n",
        "    def create_batch(self, num_task):\n",
        "        \"\"\"\n",
        "        Creates batches for support and query sets.\n",
        "\n",
        "        :param num_task: Number of tasks to create.\n",
        "        :return: Lists of support and query sets.\n",
        "        \"\"\"\n",
        "        supports, queries = [], []\n",
        "\n",
        "        for _ in range(num_task):\n",
        "            domain = random.choice(self.examples)['domain']\n",
        "            domain_examples = [e for e in self.examples if e['domain'] == domain]\n",
        "\n",
        "            if len(domain_examples) < (self.k_support + self.k_query):\n",
        "                continue  # Skip if not enough examples\n",
        "\n",
        "            selected_examples = random.sample(domain_examples, self.k_support + self.k_query)\n",
        "            supports.append(selected_examples[:self.k_support])\n",
        "            queries.append(selected_examples[self.k_support:])\n",
        "\n",
        "        return supports, queries\n",
        "\n",
        "    def create_feature_set(self, examples):\n",
        "        \"\"\"\n",
        "        Converts examples into a TensorDataset with encoded features.\n",
        "\n",
        "        :param examples: List of examples to convert.\n",
        "        :return: TensorDataset of encoded features.\n",
        "        \"\"\"\n",
        "        all_input_ids = torch.zeros((len(examples), self.max_seq_length), dtype=torch.long)\n",
        "        all_attention_mask = torch.zeros_like(all_input_ids)\n",
        "        all_segment_ids = torch.zeros_like(all_input_ids)\n",
        "        all_label_ids = torch.zeros(len(examples), dtype=torch.long)\n",
        "\n",
        "        for i, example in enumerate(examples):\n",
        "            encoded = self.tokenizer.encode_plus(example['text'], add_special_tokens=True,\n",
        "                                                 max_length=self.max_seq_length, padding='max_length',\n",
        "                                                 truncation=True, return_attention_mask=True)\n",
        "\n",
        "            all_input_ids[i] = torch.tensor(encoded['input_ids'], dtype=torch.long)\n",
        "            all_attention_mask[i] = torch.tensor(encoded['attention_mask'], dtype=torch.long)\n",
        "            all_segment_ids[i] = torch.tensor(encoded['token_type_ids'], dtype=torch.long)\n",
        "            all_label_ids[i] = LABEL_MAP[example['label']]\n",
        "\n",
        "        return TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Retrieves a task by index.\n",
        "\n",
        "        :param index: Index of the task.\n",
        "        :return: Tuple of support and query sets for the task.\n",
        "        \"\"\"\n",
        "        support_set = self.create_feature_set(self.supports[index])\n",
        "        query_set = self.create_feature_set(self.queries[index])\n",
        "        return support_set, query_set\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of tasks.\n",
        "        \"\"\"\n",
        "        return self.num_task\n"
      ]
    }
  ]
}