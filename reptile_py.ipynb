{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1dpPyrU83VR6WnvfSb2yn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mpiatek26/Fewshot_text_classification_with_meta_learning_and_BERT/blob/main/reptile_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lVXszZwIgxNx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "from transformers import BertForSequenceClassification\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import logging\n",
        "import gc\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class Learner(nn.Module):\n",
        "    \"\"\"\n",
        "    Meta Learner class for few-shot learning using BERT.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args):\n",
        "        super(Learner, self).__init__()\n",
        "        # Initialize the model with the provided arguments\n",
        "        self.init_model(args)\n",
        "\n",
        "    def init_model(self, args):\n",
        "        \"\"\"\n",
        "        Initialize the model and its components.\n",
        "        \"\"\"\n",
        "        self.num_labels = args.num_labels\n",
        "        self.outer_batch_size = args.outer_batch_size\n",
        "        self.inner_batch_size = args.inner_batch_size\n",
        "        self.outer_update_lr = args.outer_update_lr\n",
        "        self.inner_update_lr = args.inner_update_lr\n",
        "        self.inner_update_step = args.inner_update_step\n",
        "        self.inner_update_step_eval = args.inner_update_step_eval\n",
        "        self.bert_model = args.bert_model\n",
        "\n",
        "        # Load BERT model for sequence classification\n",
        "        self.model = BertForSequenceClassification.from_pretrained(self.bert_model, num_labels=self.num_labels)\n",
        "        self.outer_optimizer = Adam(self.model.parameters(), lr=self.outer_update_lr)\n",
        "\n",
        "        # Set the model to training mode by default\n",
        "        self.model.train()\n",
        "\n",
        "    def forward(self, batch_tasks, training=True):\n",
        "        \"\"\"\n",
        "        Forward pass for batch of tasks.\n",
        "        \"\"\"\n",
        "        task_accs = []\n",
        "        sum_gradients = []\n",
        "        num_task = len(batch_tasks)\n",
        "        num_inner_update_step = self.inner_update_step if training else self.inner_update_step_eval\n",
        "\n",
        "        for task_id, (support, query) in enumerate(batch_tasks):\n",
        "            # Process each task\n",
        "            fast_model, inner_optimizer = self.clone_model()\n",
        "            self.train_inner_loop(fast_model, inner_optimizer, support, num_inner_update_step, task_id)\n",
        "\n",
        "            # Evaluate the fast (adapted) model\n",
        "            task_acc = self.evaluate_task(fast_model, query)\n",
        "            task_accs.append(task_acc)\n",
        "\n",
        "            if training:\n",
        "                # Aggregate gradients from the fast model\n",
        "                self.aggregate_gradients(fast_model, sum_gradients, task_id)\n",
        "\n",
        "            # Clean up to free memory\n",
        "            del fast_model, inner_optimizer\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        if training:\n",
        "            # Update the original model\n",
        "            self.update_model(sum_gradients, num_task)\n",
        "\n",
        "        return np.mean(task_accs)\n",
        "\n",
        "    def clone_model(self):\n",
        "        \"\"\"\n",
        "        Clone the main model for fast adaptation.\n",
        "        \"\"\"\n",
        "        fast_model = deepcopy(self.model).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "        inner_optimizer = Adam(fast_model.parameters(), lr=self.inner_update_lr)\n",
        "        fast_model.train()\n",
        "        return fast_model, inner_optimizer\n",
        "\n",
        "    def train_inner_loop(self, fast_model, inner_optimizer, support, num_inner_update_step, task_id):\n",
        "        \"\"\"\n",
        "        Inner training loop for fast adaptation of the model.\n",
        "        \"\"\"\n",
        "        support_dataloader = DataLoader(support, sampler=RandomSampler(support), batch_size=self.inner_batch_size)\n",
        "        for _ in range(num_inner_update_step):\n",
        "            for batch in support_dataloader:\n",
        "                loss = self.compute_loss(fast_model, batch)\n",
        "                loss.backward()\n",
        "                inner_optimizer.step()\n",
        "                inner_optimizer.zero_grad()\n",
        "\n",
        "    def compute_loss(self, fast_model, batch):\n",
        "        \"\"\"\n",
        "        Compute loss for a batch of data.\n",
        "        \"\"\"\n",
        "        batch = tuple(t.to(fast_model.device) for t in batch)\n",
        "        input_ids, attention_mask, segment_ids, label_id = batch\n",
        "        outputs = fast_model(input_ids, attention_mask, segment_ids, labels=label_id)\n",
        "        return outputs[0]\n",
        "\n",
        "    def evaluate_task(self, fast_model, query):\n",
        "        \"\"\"\n",
        "        Evaluate the fast model on the query set.\n",
        "        \"\"\"\n",
        "        fast_model.eval()\n",
        "        query_dataloader = DataLoader(query, sampler=None, batch_size=len(query))\n",
        "        query_batch = iter(query_dataloader).next()\n",
        "        query_batch = tuple(t.to(fast_model.device) for t in query_batch)\n",
        "        q_input_ids, q_attention_mask, q_segment_ids, q_label_id = query_batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_outputs = fast_model(q_input_ids, q_attention_mask, q_segment_ids, labels=q_label_id)\n",
        "            q_logits = nn.functional.softmax(q_outputs[1], dim=1)\n",
        "            pre_label_id = torch.argmax(q_logits, dim=1).detach().cpu().numpy()\n",
        "            q_label_id = q_label_id.detach().cpu().numpy()\n",
        "\n",
        "            acc = accuracy_score(pre_label_id, q_label_id)\n",
        "            return acc\n",
        "\n",
        "    def aggregate_gradients(self, fast_model, sum_gradients, task_id):\n",
        "        \"\"\"\n",
        "        Aggregate gradients from the fast model for meta-update.\n",
        "        \"\"\"\n",
        "        meta_weights = list(self.model.parameters())\n",
        "        fast_weights = list(fast_model.parameters())\n",
        "\n",
        "        for i, (meta_params, fast_params) in enumerate(zip(meta_weights, fast_weights)):\n",
        "            gradient = meta_params - fast_params\n",
        "            if task_id == 0:\n",
        "                sum_gradients.append(gradient)\n",
        "            else:\n",
        "                sum_gradients[i] += gradient\n",
        "\n",
        "    def update_model(self, sum_gradients, num_task):\n",
        "        \"\"\"\n",
        "        Update the original model using the aggregated gradients.\n",
        "        \"\"\"\n",
        "        for i, params in enumerate(self.model.parameters()):\n",
        "            params.grad = sum_gradients[i] / float(num_task)\n",
        "\n",
        "        self.outer_optimizer.step()\n",
        "        self.outer_optimizer.zero_grad()\n",
        "\n",
        "        del sum_gradients\n",
        "        gc.collect()\n"
      ]
    }
  ]
}